{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM for classification, without and with kernels\n",
    "\n",
    "In this notebook we are going to explore the use of Support Vector Machines (SVMs) for image classification. We are going to use the famous MNIST dataset, that is a dataset of handwritten digits. We get the data from mldata.org, that is a public repository for machine learning data.\n",
    "\n",
    "The dataset consists of 70,000 images of handwritten digits (i.e., 0, 1, ... 9). Each image is 28 pixels by 28 pixels and we can think of it as a vector of 28x28 = 784 numbers. Each number is an integer between 0 and 255. For each image we have the corresponding label (i.e., 0, 1, ..., 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the required packages\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 2005949\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the dataset. 'data' contains the input, 'target' contains the label. We normalize the data by dividing each value by 255 so that each value is in [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the MNIST dataset and let's normalize the features so that each value is in [0,1]\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "# rescale the data\n",
    "X, y = mnist.data / 255., mnist.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test. We keep 500 samples in the training set. Make sure that each label is present at least 10 times\n",
    "in training. If it is not, then keep adding permutations to the initial data until this \n",
    "happens.\n",
    "\n",
    "**IMPORTANT**: if you cannot run the SVM with 500 samples or 1000 samples (see below), try with a smaller number of samples (e.g. 200 here and 400 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "['3' '0' '0' ... '5' '4' '4']\n",
      "Labels and frequencies in training dataset: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object),\n",
       " array([52, 56, 71, 43, 48, 38, 53, 52, 44, 43], dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random permute the data and split into training and test taking the first 500\n",
    "#data samples as training and the rests as test\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "print(X)\n",
    "print(y)\n",
    "m_training = 500\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "print(\"Labels and frequencies in training dataset: \")\n",
    "np.unique(y_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now provide a function to print an image in a dataset, the corresponding true label, and the index of the image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting a digit and printing the corresponding labe\n",
    "def plot_digit(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %s\" % labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's print the 100-th image in X_train and the 40,000-th image in X_test and their true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 2\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 4\n"
     ]
    }
   ],
   "source": [
    "#let's try the plotting function\n",
    "plot_digit(X_train,y_train,100)\n",
    "plot_digit(X_test,y_test,40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 1\n",
    "Run SVM with cross validation to pick a kernel and values of parameters. Use a 5-fold cross-validation to pick the best kernel and choice of parameters. We provide some potential choice for parameters, but change the grid if needed (e.g., it takes too long). For the SVM for classification use SVC from sklearn.svm; for the grid search we suggest you use GridSearchCV from sklearn.model_selection, but you can implement your own cross-validation for model selection if you prefer.\n",
    "\n",
    "Print the best parameters used as well as the score obtained by the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR LINEAR KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 1}\n",
      "Score with best parameters:\n",
      "0.87\n",
      "\n",
      "All scores on the grid:\n",
      "{'mean_fit_time': array([0.21499119, 0.21952252, 0.20584483]), 'std_fit_time': array([0.00580949, 0.00937339, 0.00795787]), 'mean_score_time': array([0.05025945, 0.05238342, 0.04923792]), 'std_score_time': array([0.00385867, 0.00390589, 0.00194746]), 'param_C': masked_array(data=[1, 10, 100],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1}, {'C': 10}, {'C': 100}], 'split0_test_score': array([0.88571429, 0.88571429, 0.88571429]), 'split1_test_score': array([0.91262136, 0.91262136, 0.91262136]), 'split2_test_score': array([0.87128713, 0.87128713, 0.87128713]), 'split3_test_score': array([0.86458333, 0.86458333, 0.86458333]), 'split4_test_score': array([0.81052632, 0.81052632, 0.81052632]), 'mean_test_score': array([0.87, 0.87, 0.87]), 'std_test_score': array([0.0332279, 0.0332279, 0.0332279]), 'rank_test_score': array([1, 1, 1]), 'split0_train_score': array([1., 1., 1.]), 'split1_train_score': array([1., 1., 1.]), 'split2_train_score': array([1., 1., 1.]), 'split3_train_score': array([1., 1., 1.]), 'split4_train_score': array([1., 1., 1.]), 'mean_train_score': array([1., 1., 1.]), 'std_train_score': array([0., 0., 0.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS FOR POLY DEGREE=2 KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "Score with best parameters:\n",
      "0.862\n",
      "\n",
      "All scores on the grid:\n",
      "{'mean_fit_time': array([0.30629058, 0.27000904, 0.27107716, 0.27357101, 0.27398758,\n",
      "       0.27758002, 0.27269492, 0.28113589, 0.28054643]), 'std_fit_time': array([0.01010541, 0.00738846, 0.00755522, 0.01333393, 0.0089404 ,\n",
      "       0.00712646, 0.00804594, 0.00608977, 0.00986864]), 'mean_score_time': array([0.05891991, 0.0545464 , 0.05208468, 0.05226812, 0.0547555 ,\n",
      "       0.05321498, 0.05236578, 0.05285778, 0.05415955]), 'std_score_time': array([0.00216478, 0.00475899, 0.00258195, 0.00273221, 0.00242221,\n",
      "       0.00266118, 0.00207985, 0.00339747, 0.00343761]), 'param_C': masked_array(data=[1, 1, 1, 10, 10, 10, 100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[0.01, 0.1, 1.0, 0.01, 0.1, 1.0, 0.01, 0.1, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}], 'split0_test_score': array([0.85714286, 0.84761905, 0.84761905, 0.85714286, 0.84761905,\n",
      "       0.84761905, 0.84761905, 0.84761905, 0.84761905]), 'split1_test_score': array([0.88349515, 0.90291262, 0.90291262, 0.90291262, 0.90291262,\n",
      "       0.90291262, 0.90291262, 0.90291262, 0.90291262]), 'split2_test_score': array([0.87128713, 0.87128713, 0.87128713, 0.87128713, 0.87128713,\n",
      "       0.87128713, 0.87128713, 0.87128713, 0.87128713]), 'split3_test_score': array([0.84375   , 0.88541667, 0.88541667, 0.88541667, 0.88541667,\n",
      "       0.88541667, 0.88541667, 0.88541667, 0.88541667]), 'split4_test_score': array([0.77894737, 0.78947368, 0.78947368, 0.78947368, 0.78947368,\n",
      "       0.78947368, 0.78947368, 0.78947368, 0.78947368]), 'mean_test_score': array([0.848, 0.86 , 0.86 , 0.862, 0.86 , 0.86 , 0.86 , 0.86 , 0.86 ]), 'std_test_score': array([0.03600106, 0.03881184, 0.03881184, 0.03836512, 0.03881184,\n",
      "       0.03881184, 0.03881184, 0.03881184, 0.03881184]), 'rank_test_score': array([9, 2, 2, 1, 2, 2, 2, 2, 2]), 'split0_train_score': array([0.96202532, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split1_train_score': array([0.94458438, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split2_train_score': array([0.96491228, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split3_train_score': array([0.96287129, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split4_train_score': array([0.95308642, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'mean_train_score': array([0.95749594, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'std_train_score': array([0.00762317, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS FOR rbf KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "Score with best parameters:\n",
      "0.882\n",
      "\n",
      "All scores on the grid:\n",
      "{'mean_fit_time': array([0.34282188, 0.52742658, 0.53624363, 0.38024158, 0.51640096,\n",
      "       0.52657685, 0.38012528, 0.51747403, 0.52225575]), 'std_fit_time': array([0.008936  , 0.01186422, 0.01225446, 0.0124836 , 0.01201077,\n",
      "       0.00859602, 0.00560592, 0.01747606, 0.0146838 ]), 'mean_score_time': array([0.06293273, 0.0715436 , 0.06920371, 0.06192961, 0.07081656,\n",
      "       0.07220736, 0.0638701 , 0.0704011 , 0.07060504]), 'std_score_time': array([0.00409127, 0.00260087, 0.00345178, 0.0045065 , 0.00552479,\n",
      "       0.00265325, 0.00484451, 0.00184287, 0.00397177]), 'param_C': masked_array(data=[1, 1, 1, 10, 10, 10, 100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[0.01, 0.1, 1.0, 0.01, 0.1, 1.0, 0.01, 0.1, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}], 'split0_test_score': array([0.86666667, 0.33333333, 0.14285714, 0.86666667, 0.40952381,\n",
      "       0.14285714, 0.86666667, 0.40952381, 0.14285714]), 'split1_test_score': array([0.93203883, 0.32038835, 0.13592233, 0.95145631, 0.36893204,\n",
      "       0.13592233, 0.95145631, 0.36893204, 0.13592233]), 'split2_test_score': array([0.87128713, 0.27722772, 0.13861386, 0.89108911, 0.32673267,\n",
      "       0.13861386, 0.89108911, 0.32673267, 0.13861386]), 'split3_test_score': array([0.875     , 0.32291667, 0.14583333, 0.88541667, 0.34375   ,\n",
      "       0.14583333, 0.88541667, 0.34375   , 0.14583333]), 'split4_test_score': array([0.83157895, 0.31578947, 0.14736842, 0.81052632, 0.31578947,\n",
      "       0.14736842, 0.81052632, 0.31578947, 0.14736842]), 'mean_test_score': array([0.876, 0.314, 0.142, 0.882, 0.354, 0.142, 0.882, 0.354, 0.142]), 'std_test_score': array([0.03232333, 0.01938862, 0.00428681, 0.04508543, 0.03378028,\n",
      "       0.00428681, 0.04508543, 0.03378028, 0.00428681]), 'rank_test_score': array([3, 6, 7, 1, 4, 7, 1, 4, 7]), 'split0_train_score': array([0.97974684, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split1_train_score': array([0.97732997, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split2_train_score': array([0.98746867, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split3_train_score': array([0.97772277, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split4_train_score': array([0.98271605, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'mean_train_score': array([0.98099686, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'std_train_score': array([0.00375681, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ])}\n"
     ]
    }
   ],
   "source": [
    "#import SVC\n",
    "from sklearn.svm import SVC\n",
    "#import for Cross-Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters for linear SVM\n",
    "parameters = {'C': [1, 10, 100]}\n",
    "\n",
    "#run linear SVM\n",
    "linear_SVM = SVC(kernel='linear')\n",
    "\n",
    "\n",
    "#find best model uusing 5-fold CV \n",
    "#and train it using all the training data\n",
    "# ADD CODE\n",
    "#cv default value if None, changed from 3-fold to 5-fold.\n",
    "cross_val = GridSearchCV(estimator=linear_SVM, param_grid =parameters, cv = 5)\n",
    "cross_val.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL\\n')\n",
    " \n",
    "print(\"Best parameters set found:\")\n",
    "# ADD CODE\n",
    "print(cross_val.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "# ADD CODE\n",
    "print(cross_val.best_score_)\n",
    "\n",
    "    \n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(cross_val.cv_results_)\n",
    "\n",
    "\n",
    "# parameters for poly with degree 2 kernel\n",
    "parameters = {'C': [1, 10, 100],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "#run SVM with poly of degree 2 kernel\n",
    "poly2_SVM = SVC(kernel='poly',degree=2)\n",
    "\n",
    "# ADD CODE: DO THE SAME AS ABOVE FOR POLYNOMIAL KERNEL WITH DEGREE=2\n",
    "\n",
    "cross_val2 = GridSearchCV(estimator=poly2_SVM, param_grid =parameters, cv = 5)\n",
    "\n",
    "cross_val2.fit(X_train, y_train)\n",
    "\n",
    "print ('\\nRESULTS FOR POLY DEGREE=2 KERNEL\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "# ADD CODE\n",
    "print(cross_val2.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(cross_val2.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "# ADD CODE\n",
    "print(cross_val2.cv_results_)\n",
    "\n",
    "# parameters for rbf SVM\n",
    "parameters = {'C': [1, 10, 100],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "#run SVM with rbf kernel\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "cross_val3 = GridSearchCV(estimator=rbf_SVM, param_grid =parameters, cv = 5)\n",
    "\n",
    "cross_val3.fit(X_train, y_train)\n",
    "\n",
    "# ADD CODE: DO THE SAME AS ABOVE FOR RBF KERNEL\n",
    "\n",
    "print ('\\nRESULTS FOR rbf KERNEL\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(cross_val3.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(cross_val3.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(cross_val3.cv_results_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 2\n",
    "For the \"best\" SVM kernel and choice of parameters from above, train the model on the entire training set and measure the training error. Also make predictions on the test set and measure the test error. Print the training and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM training error: 0.160000\n",
      "Best SVM test error: 0.231468\n"
     ]
    }
   ],
   "source": [
    "#get training and test error for the best SVM model from CV\n",
    "parameters = {'C': [10],'gamma':[0.01]}\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "best_SVM = rbf_SVM\n",
    "\n",
    "# fit the model on the entire training set\n",
    "# ADD CODE\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "#get the training and test error\n",
    "training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use logistic regression for comparison\n",
    "\n",
    "## TO DO 3\n",
    "\n",
    "Just for comparison let's also use logistic regression, first with the default values of the parameter for regularization and then with cross-validation to fix the value of the parameter. For cross validation, use 5-fold cross validation and the default values of the regularization parameters for the function linear_model.LogisticRegressionCV(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression training error: 0.000000\n",
      "Best logistic regression test error: 0.163755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression training error: 0.000000\n",
      "Best logistic regression test error: 0.172446\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lr = linear_model.LogisticRegression()\n",
    "# fit the model on the training data\n",
    "# ADD CODE\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#compute training and test error for model above\n",
    "training_error = 1 - lr.score(X_train,y_train)\n",
    "test_error = 1 - lr.score(X_test,y_test)\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error)\n",
    "print (\"Best logistic regression test error: %f\" % test_error)\n",
    "\n",
    "#logistic regression with 5-fold CV: you can use use linear_model.LogisticRegressionCV\n",
    "# use 5-fold CV to find the best choice of the parameter, than train\n",
    "# the model on the entire training set\n",
    "#Changed in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n",
    "lr_cv = linear_model.LogisticRegressionCV(cv = 5)\n",
    "lr_cv.fit(X_train,y_train)\n",
    "training_error_cv = 1 - lr_cv.score(X_train, y_train)\n",
    "test_error_cv = 1 - lr_cv.score(X_test, y_test)\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error_cv)\n",
    "print (\"Best logistic regression test error: %f\" % test_error_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 4 \n",
    "Compare and comment the results from SVM and logistic regression.\n",
    "\n",
    "\n",
    "With SVM rbf kernel, which is the best one according to the results, I have had the training error=0.16 whereas the test_error= 0.23, with logistic regression both the training errors are 0.0 and the test error results are lower than rbf svm, which is respectively 0.1637 and 0.172.\n",
    "I think that maybe if we had more data, it would make more sense choosing rbf svm, with respect to the error values. For logistic regression, in my opinion the data size is too low therefore the cross validation one has a higher error compared to the other logistic regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 5\n",
    "Write the code that finds and plots a digit that is missclassified by logistic regression (optimized for the regularization parameter) and correctly classified by the \"best\" SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label :  ['0'] RBF SVM Prediction: ['0'] Logistic Regression Prediction :  ['3']\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: ['0']\n"
     ]
    }
   ],
   "source": [
    "# ADD CODE\n",
    "prediction_for_lr = lr.predict(X_test)  #to get the values which logistic regression predicted in order to compare the actual labels\n",
    "pred_lr = (prediction_for_lr == y_test)  #comparing the actual labels and the ones founded by lr\n",
    "\n",
    "prediction_for_svm = best_SVM.predict(X_test)  #to get the values which RBF SVM predicted in order to compare the actual label\n",
    "pred_SVM = (prediction_for_svm == y_test)    #comparing the actual labels and the ones founded by rbf kernel\n",
    "  \n",
    "misclassified_for_lr = np.asarray(np.where(pred_lr == False))  #misclassified ones for logistic regression\n",
    "correctly_classified_rbf_svm = np.asarray(np.where(pred_SVM == True))  #correclty classified ones for SVM\n",
    "\n",
    "goal = np.in1d(misclassified_for_lr,correctly_classified_rbf_svm) #goal is to find the ones correclty classified for svm and misclassified by lr\n",
    "indexes_of_goals = np.where(goal == True)  #indexes of the values which is found\n",
    "\n",
    "mlr_csvm = np.take(misclassified_for_lr, indexes_of_goals) #the ones which has been missclassified by logistic regression, but correctly classified with svm\n",
    "mlr_csvm = mlr_csvm.reshape((len(mlr_csvm[0])), len(mlr_csvm))\n",
    "index = mlr_csvm[1]\n",
    "print(\"Original Label : \",y_test[index], \"RBF SVM Prediction:\", prediction_for_svm[index], \"Logistic Regression Prediction : \", prediction_for_lr[index] )\n",
    "plot_digit(X_test,y_test,index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data\n",
    "Now let's do the same but using 1000 data points for training. \n",
    "\n",
    "## TO DO 6\n",
    "Repeat the entire analysis above using 1000 samples. Of course you can copy the code from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 7\n",
    "Compare and comment on the differences with the results above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and result:\n",
    "\n",
    "For 1000 samples, the results are better for all the models. For linear, poly of degree 2, and rbf kernel SVMs, the best scores were 0.87, 0.862. 0.882, after making the samples 1000, the best scores went up to the numbers: 0.884,0.907,0.923 which was a clear improvement for all these SVMs with different kernels. The best SVM remained the same as rbf kernel and for rbf kernel the training and test error were:0.16 and 0.23. After increasing the sample count the training and test error became: 0.09 and 0.134, which is also a visible improvement. For logistic regression the first one had the training and test error: 0.00 , 0.163 and the second one had the training error:0.00 and 0.17.\n",
    "After increasing the total number of samples, the training and test error has improved as well too: first one: 0.00    0.134  second one: 0.00  0.144.\n",
    "But still with 1000 samples logistic regression performed better than the best SVM which is rbf.\n",
    "In my opinion if the number of samples keep increasing, then the SVM's will outperform the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR LINEAR KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 1}\n",
      "Score with best parameters:\n",
      "0.884\n",
      "\n",
      "All scores on the grid:\n",
      "{'mean_fit_time': array([0.73971148, 0.75932922, 0.72617531]), 'std_fit_time': array([0.02972851, 0.03920526, 0.02306823]), 'mean_score_time': array([0.23215013, 0.21656942, 0.21828938]), 'std_score_time': array([0.01316546, 0.01534826, 0.01000786]), 'param_C': masked_array(data=[1, 10, 100],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1}, {'C': 10}, {'C': 100}], 'split0_test_score': array([0.91666667, 0.91666667, 0.91666667]), 'split1_test_score': array([0.86633663, 0.86633663, 0.86633663]), 'split2_test_score': array([0.87, 0.87, 0.87]), 'split3_test_score': array([0.88944724, 0.88944724, 0.88944724]), 'split4_test_score': array([0.87692308, 0.87692308, 0.87692308]), 'mean_test_score': array([0.884, 0.884, 0.884]), 'std_test_score': array([0.01831897, 0.01831897, 0.01831897]), 'rank_test_score': array([1, 1, 1]), 'split0_train_score': array([1., 1., 1.]), 'split1_train_score': array([1., 1., 1.]), 'split2_train_score': array([1., 1., 1.]), 'split3_train_score': array([1., 1., 1.]), 'split4_train_score': array([1., 1., 1.]), 'mean_train_score': array([1., 1., 1.]), 'std_train_score': array([0., 0., 0.])}\n",
      "\n",
      "RESULTS FOR POLY DEGREE=2 KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "Score with best parameters:\n",
      "0.907\n",
      "\n",
      "All scores on the grid:\n",
      "{'mean_fit_time': array([1.14615374, 0.95598669, 0.9336802 , 0.93556323, 0.89424906,\n",
      "       0.94210377, 0.93791161, 0.93546686, 0.91867762]), 'std_fit_time': array([0.03412605, 0.06043408, 0.04577031, 0.04133593, 0.02368869,\n",
      "       0.04969409, 0.03885319, 0.02231938, 0.03195896]), 'mean_score_time': array([0.25368619, 0.21502218, 0.21419158, 0.21730142, 0.21448212,\n",
      "       0.21670008, 0.21154056, 0.21693983, 0.21991773]), 'std_score_time': array([0.00880673, 0.00717605, 0.02067906, 0.01332152, 0.0105888 ,\n",
      "       0.00605258, 0.00829513, 0.00611974, 0.01216586]), 'param_C': masked_array(data=[1, 1, 1, 10, 10, 10, 100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[0.01, 0.1, 1.0, 0.01, 0.1, 1.0, 0.01, 0.1, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}], 'split0_test_score': array([0.89215686, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667]), 'split1_test_score': array([0.85643564, 0.87128713, 0.87128713, 0.87128713, 0.87128713,\n",
      "       0.87128713, 0.87128713, 0.87128713, 0.87128713]), 'split2_test_score': array([0.86 , 0.915, 0.915, 0.915, 0.915, 0.915, 0.915, 0.915, 0.915]), 'split3_test_score': array([0.89447236, 0.91457286, 0.91457286, 0.91959799, 0.91457286,\n",
      "       0.91457286, 0.91457286, 0.91457286, 0.91457286]), 'split4_test_score': array([0.87179487, 0.90769231, 0.90769231, 0.91282051, 0.90769231,\n",
      "       0.90769231, 0.90769231, 0.90769231, 0.90769231]), 'mean_test_score': array([0.875, 0.905, 0.905, 0.907, 0.905, 0.905, 0.905, 0.905, 0.905]), 'std_test_score': array([0.01587841, 0.01723372, 0.01723372, 0.01810207, 0.01723372,\n",
      "       0.01723372, 0.01723372, 0.01723372, 0.01723372]), 'rank_test_score': array([9, 2, 2, 1, 2, 2, 2, 2, 2]), 'split0_train_score': array([0.96859296, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split1_train_score': array([0.96741855, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split2_train_score': array([0.96875, 1.     , 1.     , 1.     , 1.     , 1.     , 1.     ,\n",
      "       1.     , 1.     ]), 'split3_train_score': array([0.96004994, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split4_train_score': array([0.96770186, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'mean_train_score': array([0.96650266, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'std_train_score': array([0.00326602, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS FOR rbf KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "Score with best parameters:\n",
      "0.923\n",
      "\n",
      "All scores on the grid:\n",
      "{'mean_fit_time': array([1.2672493 , 2.61418629, 2.85145483, 1.31271811, 2.84702377,\n",
      "       2.7264843 , 1.22550783, 2.63095174, 2.62498398]), 'std_fit_time': array([0.04071425, 0.04884428, 0.13523966, 0.02259882, 0.07767057,\n",
      "       0.06728213, 0.02940438, 0.05001711, 0.03252283]), 'mean_score_time': array([0.28983078, 0.34434981, 0.37783294, 0.28453851, 0.37191062,\n",
      "       0.37189837, 0.26730485, 0.33761253, 0.36083999]), 'std_score_time': array([0.01176322, 0.01014732, 0.01729524, 0.01383929, 0.03532785,\n",
      "       0.00219413, 0.01724325, 0.01486717, 0.01015581]), 'param_C': masked_array(data=[1, 1, 1, 10, 10, 10, 100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[0.01, 0.1, 1.0, 0.01, 0.1, 1.0, 0.01, 0.1, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 1.0}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 1.0}, {'C': 100, 'gamma': 0.01}, {'C': 100, 'gamma': 0.1}, {'C': 100, 'gamma': 1.0}], 'split0_test_score': array([0.92156863, 0.4754902 , 0.12745098, 0.93627451, 0.52941176,\n",
      "       0.12745098, 0.93627451, 0.52941176, 0.12745098]), 'split1_test_score': array([0.89108911, 0.42079208, 0.12871287, 0.91089109, 0.46534653,\n",
      "       0.12871287, 0.91089109, 0.46534653, 0.12871287]), 'split2_test_score': array([0.895, 0.38 , 0.125, 0.91 , 0.46 , 0.125, 0.91 , 0.46 , 0.125]), 'split3_test_score': array([0.91959799, 0.39698492, 0.12562814, 0.92462312, 0.44723618,\n",
      "       0.12562814, 0.92462312, 0.44723618, 0.12562814]), 'split4_test_score': array([0.91282051, 0.41538462, 0.12820513, 0.93333333, 0.45128205,\n",
      "       0.12820513, 0.93333333, 0.45128205, 0.12820513]), 'mean_test_score': array([0.908, 0.418, 0.127, 0.923, 0.471, 0.127, 0.923, 0.471, 0.127]), 'std_test_score': array([0.0126659 , 0.03246288, 0.00144633, 0.01098689, 0.03024707,\n",
      "       0.00144633, 0.01098689, 0.03024707, 0.00144633]), 'rank_test_score': array([3, 6, 7, 1, 4, 7, 1, 4, 7]), 'split0_train_score': array([0.98115578, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split1_train_score': array([0.97619048, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split2_train_score': array([0.9825, 1.    , 1.    , 1.    , 1.    , 1.    , 1.    , 1.    ,\n",
      "       1.    ]), 'split3_train_score': array([0.98127341, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'split4_train_score': array([0.97763975, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'mean_train_score': array([0.97975188, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'std_train_score': array([0.00240761, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irems\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "m_training = 1000\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "#import SVC\n",
    "from sklearn.svm import SVC\n",
    "#import for Cross-Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters for linear SVM\n",
    "parameters = {'C': [1, 10, 100]}\n",
    "\n",
    "#run linear SVM\n",
    "linear_SVM = SVC(kernel='linear')\n",
    "\n",
    "\n",
    "#find best model uusing 5-fold CV \n",
    "#and train it using all the training data\n",
    "# ADD CODE\n",
    "#cv default value if None, changed from 3-fold to 5-fold.\n",
    "cross_val = GridSearchCV(estimator=linear_SVM, param_grid =parameters, cv = 5)\n",
    "cross_val.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL\\n')\n",
    " \n",
    "print(\"Best parameters set found:\")\n",
    "# ADD CODE\n",
    "print(cross_val.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "# ADD CODE\n",
    "print(cross_val.best_score_)\n",
    "\n",
    "    \n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(cross_val.cv_results_)\n",
    "\n",
    "\n",
    "# parameters for poly with degree 2 kernel\n",
    "parameters = {'C': [1, 10, 100],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "#run SVM with poly of degree 2 kernel\n",
    "poly2_SVM = SVC(kernel='poly',degree=2)\n",
    "\n",
    "# ADD CODE: DO THE SAME AS ABOVE FOR POLYNOMIAL KERNEL WITH DEGREE=2\n",
    "\n",
    "cross_val2 = GridSearchCV(estimator=poly2_SVM, param_grid =parameters, cv = 5)\n",
    "\n",
    "cross_val2.fit(X_train, y_train)\n",
    "\n",
    "print ('\\nRESULTS FOR POLY DEGREE=2 KERNEL\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "# ADD CODE\n",
    "print(cross_val2.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(cross_val2.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "# ADD CODE\n",
    "print(cross_val2.cv_results_)\n",
    "\n",
    "# parameters for rbf SVM\n",
    "parameters = {'C': [1, 10, 100],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "#run SVM with rbf kernel\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "cross_val3 = GridSearchCV(estimator=rbf_SVM, param_grid =parameters, cv = 5)\n",
    "\n",
    "cross_val3.fit(X_train, y_train)\n",
    "\n",
    "# ADD CODE: DO THE SAME AS ABOVE FOR RBF KERNEL\n",
    "\n",
    "print ('\\nRESULTS FOR rbf KERNEL\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(cross_val3.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(cross_val3.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(cross_val3.cv_results_) \n",
    "\n",
    "#get training and test error for the best SVM model from CV\n",
    "parameters = {'C': [10],'gamma':[0.01]}\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "best_SVM = rbf_SVM\n",
    "\n",
    "# fit the model on the entire training set\n",
    "# ADD CODE\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "#get the training and test error\n",
    "training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)\n",
    "from sklearn import linear_model\n",
    "\n",
    "lr = linear_model.LogisticRegression()\n",
    "# fit the model on the training data\n",
    "# ADD CODE\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#compute training and test error for model above\n",
    "training_error = 1 - lr.score(X_train,y_train)\n",
    "test_error = 1 - lr.score(X_test,y_test)\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error)\n",
    "print (\"Best logistic regression test error: %f\" % test_error)\n",
    "\n",
    "#logistic regression with 5-fold CV: you can use use linear_model.LogisticRegressionCV\n",
    "# use 5-fold CV to find the best choice of the parameter, than train\n",
    "# the model on the entire training set\n",
    "#Changed in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n",
    "lr_cv = linear_model.LogisticRegressionCV(cv = 5)\n",
    "lr_cv.fit(X_train,y_train)\n",
    "training_error_cv = 1 - lr_cv.score(X_train, y_train)\n",
    "test_error_cv = 1 - lr_cv.score(X_test, y_test)\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error_cv)\n",
    "print (\"Best logistic regression test error: %f\" % test_error_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
